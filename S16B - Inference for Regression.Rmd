---
title: "Activity 16B - Inference for Regression"
author: "STS2720 (Dr. VanKrevelen)"
date: 'Updated: 11/9/2021'
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

We will use the following R packages during today's activity.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
library(dplyr)
library(Lock5Data)
library(moderndive)
```

---

The goal of today's activity is to consider how we can use hypothesis tests and confidence intervals with regression. This can be done for our coefficients (i.e. the intercept or slope(s)) or for a prediction. In multiple linear regression, we can use hypothesis tests to decide which variables to use for our explanatory variables.

Let's continue using the dataset called `SampCountries2e` from the `Lock5Data` R package. Recall that the data is a sample of data from 50 countries (in 2014). There are a total of 25 variables, but I've narrowed it to just a few below and shortened the name to `countries`.

```{r}
countries <- SampCountries2e %>%
  select(Country, LifeExpectancy, Population, Internet, BirthRate, CO2)
```

Here is a recap of the variables left in the data

* `Country` - Name of the country
* `LifeExpectancy` - Average life expectancy (in years)
* `Population` - Population (in millions)
* `Internet` - Percentage of the population with access to the internet
* `BirthRate` - Births per 1,000 people
* `CO2` - CO2 emissions (in metric tons per capita)

---

# Hypothesis Tests for a slope

Recall that when we first talked about simple linear regression, we discussed a "true model" and an "estimated model".


<br>

True model: $y = \beta_0 + \beta_1 x$

Estimated model: $\hat{y} = b_0 + b_1 x$

<br>


One thing of interest to us is whether there is even a linear relationship between our variables. In other words, we may want to test:


<br>

$H_0: \beta_1 = 0$ (no linear relationship between x and y)

$H_a: \beta_1 \ne 0$ (some sort of linear relationship between x and y)

<br>


1. Let's create a simple linear regression model to predict life expectancy based on CO2 emissions. Store the model in an object called `co2.lm` and use the `get_regression_table()` function on that object. (Note: The `get_regression_table()` function is part of the `moderndive` package that goes along with our textbook. The `summary()` function provides similar output with slightly less detail.)

```{r}
co2.lm <- lm(LifeExpectancy ~ CO2, data = countries)
get_regression_table(co2.lm)
```

<br>

2. What is our estimate of the true slope to predict life expectancy for a country based on its CO2 emissions?

**$b_1 = -0.841$**


<br>

3. Notice the second column is titled Std. Error. What do you think that number tells us?

**The standard error is how much we think our estimate would vary from sample to sample (like a standard deviation for our estimate). Think of this as the standard deviation in a bootstrap distribution.**


<br>

4. The fourth column is a p-value for the hypothesis test mentioned above. What is our p-value and what conclusion would we make for the hypothesis test we discussed?

**Our p-value rounds to 0, which tells us that we can reject the null hypothesis (our two variables are not related) and we have VERY STRONG evidence that the true slope for CO2 is not 0.**


<br>

5. Create a simple linear regression model using `Population` to predict `LifeExpectancy`. What sort of conclusion would you reach if you do a hypothesis test for the slope in this example?

```{r}
pop.lm <- lm(LifeExpectancy ~ Population, data = countries)
get_regression_table(pop.lm)
```


**Our p-value is VERY LARGE (0.905), which suggests that a slope like this is very consistent with what we'd see if our null model were correct (and population did not have a linear relationship with life expectancy). There is almost no evidence to suggest the true slope for population is not 0, so we wouldn't want to use it to predict life expectancy.**


<br>
<br>

## Background for this hypothesis test

This test is done using a theory-based method. For this method, we calculate:

$$t = \frac{b_1}{\sqrt{MSE} \sqrt{\frac{1}{(n-1)s_x^2}}}$$

and then compare the value we get to a T distribution with $n - 2$ degree of freedom. That is because if the null hypothesis (and a couple other conditions) are true, the test statistics from many different random samples would make this shape for their distribution.

You can see [Section 4 of Chapter 10](https://moderndive.com/10-inference-for-regression.html#infer-regression) for an example of how to use the `infer` package if you'd like.

---

# Confidence Intervals for a slope

We could make a confidence interval for our slope if we wanted to be able to communicate how much certainty/uncertainty there was in our estimate of the true slope.


6. The estimated slope associated with CO2 was -0.841. Write a sentence interpreting this number in context of this example.

**The predicted life expectancy of a country decreases by 0.841 years for each additional metric ton of CO2 emissions per capita.**


<br>

7. Use the `get_regression_table()` function on our `co2.lm` object again. This time pay attention to lower_ci and upper_ci columns. These are the endpoints to a 95% confidence interval for our estimate of the true slope. Write a sentence interpreting this confidence interval in context of this example.

```{r}
get_regression_table(co2.lm)
```

**We are 95% confident that the predicted life expectancy of a country decreases by between 0.673 and 1.01 years for each additional metric ton of CO2 emissions per capita.**


<br>
<br>

## Background for this interval

This confidence interval is based on the formula

$$b_1 \pm t^*_{1-\frac{\alpha}{2}, n-2} \sqrt{MSE}\sqrt{\frac{1}{(n-1)s_x^2}}$$

Don't worry about the actual formula as much as noticing that it follows our typical

$$\text{estimate} \pm \text{critical value} * \text{standard error}$$

format we talked about previously. 

- $b_1$ is our estimate of the true slope.
- $t^*_{1-\frac{\alpha}{2}, n-2}$ is a critical value to control how confident we want to be that our interval contains $\beta_1$
- $\sqrt{MSE}\sqrt{\frac{1}{(n-1)s_x^2}}$ is the standard error that measures how much our estimate typically varies from sample to sample

---

# Confidence/Prediction Intervals for a Predicted Value

In addition to making confidence intervals for our slope, we can create confidence intervals for a single point on our line too.

I've added code below to create a scatter plot of `CO2` versus `LifeExpectancy`. The `geom_smooth()` function along with the `method = "lm"` argument is used to add a simple linear regression line to the graph.

```{r}
ggplot(countries, aes(x = CO2, y = LifeExpectancy)) +
  geom_point() +
  geom_smooth(method = "lm")
```

The shaded area around the line is a 95% confidence interval for the population mean response at each value of `CO2`. 

<br>

8. Use the `predict.lm()` function to predict the life expectancy for a country with 30 metric tons of CO2 emissions per capita. I've gotten you started. What is our prediction?

```{r, eval = TRUE}
predict.lm(co2.lm, newdata = data.frame(CO2 = 30))
```

**62.7 years**


<br>

9. Run the code above again but this time add a new argument of `interval = "confidence"`. This will give us a 95% confidence interval for the population mean. Does it look like it matches what you see on the graph?

```{r, eval = TRUE}
predict.lm(co2.lm, newdata = data.frame(CO2 = 30), interval = "confidence")
```

**95% CI for population mean life expectancy when CO2 = 30: (60.45, 65.00)**

**This does match the graph which shows a shaded area (at x = 30) from y values of just over 60 to right around 65**


<br>

10. Does it look like 95% of the points fall inside our 95% confidence interval? Why do you think that might be?

**No! This is a confidence interval for the population mean life expectancy for various levels of CO2. That does NOT mean 95% of individual countries will have values in this range.**


<br>

11. A prediction interval is an interval for a specific individual/observation (rather than for a population mean). Run the code from question 9, but change `interval = "confidence"` to `interval = "prediction"`. How does the prediction interval compare to the confidence interval? Why do you think that is?

```{r, eval = TRUE}
predict.lm(co2.lm, newdata = data.frame(CO2 = 30), interval = "prediction")
```

**95% Prediction Interval when C2 = 30: (50.48, 74.97)**

**This interval is quite a bit wider. This is because it's much harder to predict the life expectancy for a single country than to predict the average life expectancy of many countries with the same CO2. Think of this like trying to predict the height of the next person to enter a building versus predicting the average height of everyone to enter the building**


<br>

12. Interpret the interval above. Remember that this is an interval for an *individual observation* instead of a *parameter*.

**We are 95% confident that the life expectancy is between 50.48 and 74.97 years for a specific country that has 30 metric tons of CO2 emissions**


<br>

---

# Using Hypothesis Tests for MLR

In our previous activity, we created a multiple linear regression model to predict life expectancy. I've run the model below and displayed the results using the `summary()` function.

```{r}
countries_mlr <- lm(LifeExpectancy ~ Internet + CO2 + BirthRate, data = countries)
summary(countries_mlr)
```

Notice that some of our explanatory variables have p-values next to their estimated slopes that indicate there isn't sufficient evidence they have a linear relationship with our response (once we've accounted for the other explanatory variables). This means we may not need them in our model.

<br>

**Backwards selection** is a method for choosing the "best" multiple linear regression model to use. 

- To start, we fit a multiple linear regression model where many explanatory variables are used to predict a response.
- If any of our slopes have p-values above a certain threshhold (say $\alpha = 0.05$ or $\alpha = 0.1$), we remove the variable with the *highest* p-value and re-fit our model.
- We will repeat this process until all of the explanatory variables in our model have p-values below a desired threshhold

<br>

13. Let's try using backwards selection with our countries model. Which variable should we remove first?

**We will start by removing CO2 from our model because it has the biggest p-value (that is also above 0.05). Essentially this means we are not convinced the true slope is something other than 0, so CO2 really might not be helping our prediction much.**



<br>

14. Remove that variable and re-fit the model. Are there any other variables we should remove?

```{r}
countries_mlr2 <- lm(LifeExpectancy ~ Internet + BirthRate, data = countries)
summary(countries_mlr2)
```

**After removing CO2, the p-values for the two remaining explanatory variables are both VERY SMALL, which indicates that they will be helpful in predicting life expectancy. In other words, we have very strong evidence that the true slopes are NOT 0 for these two variables in our model.**

<br>

15. What do you notice about the estimated slope for `BirthRate` from our first model to our second? Why do you think that happened?

**The estimated slope went from -2.33 to -0.59. While both are negative, these are pretty different. However, in the first model we had a large standard error because we weren't very confident in that -2.33 value (due to CO2 also being in our model). When we removed CO2, we became much more confident that the slope for BirthRate was close to -.6**


<br>

16. How did the R-squared value change?

**The $R^2$ value went down because within our data, we can never do worse making a prediction with more variables. However, the change was small (CO2 didn't help much once we had the other two explanatory variables), and the Adjusted $R^2$ actually went up. The adjusted $R^2$ will penalize a model for having more explanatory variables because smaller models are easier to interpret.**


**Note: I said above that we can never do worse making a prediction with more variables, but this is only true when we are making a prediction on the same data that created the model. It is absolutely possible that adding unrelated variables can make our predictions worse on NEW data. Researchers will sometimes split their data into a "training data set" and a "test data set". They will then create a model using the training data but test how well it does in predicting the test data.**

<br>
