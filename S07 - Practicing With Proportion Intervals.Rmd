---
title: "Activity 07 - Practicing With Proportion Intervals"
author: "STS2720 (Dr. VanKrevelen)"
date: 'Updated: 9/19/2021'
output:
  html_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(ggplot2)
library(infer)
library(dplyr)
```

For this example, we'll look at a study where researchers trained a dog to smell COVID-19 virus. You can read more about it (and watch a video) [here](https://nerdist.com/article/dogs-sniff-out-covid/).

```{r, out.width = '75%', fig.align = 'center', echo = FALSE, fig.cap = 'Sniffer dog at work'}
knitr::include_graphics('https://nerdist.com/wp-content/uploads/2020/07/C19-sniffing-dogs-feature-image-07272020.jpg')
```

# Part 1 - A single proportion

1. I've created a dataset out of the data in the article. You can see it below. How many observations are there? What are the variables and what does each one mean?

```{r}
covid_dog <- data.frame(ID = c(rep("positive", 157), rep("negative", 792),
                               rep("positive", 33), rep("negative", 30)),
                        actual = c(rep("positive", 157), rep("negative", 792),
                                   rep("negative", 33), rep("positive", 30))) %>%
  mutate(correct = factor(ID == actual))
```

**There are 1,012 observations. Each observation measures three variables: ID (what the dog determined), actual (whether COVID-19 was present in the sample), and correct (whether the dog's ID matches the correct ID)**

<br>


2. Let's start by just looking at cases where the sample was actually positive for COVID-19. Use the `filter()` function to create a new object called `pos_samples` that only includes rows where the actual sample was positive.

```{r}
pos_samples <- filter(covid_dog, actual == "positive")
```

<br>

3. What proportion of positive tests did the dog correctly identify? How can we calculate this in R? (Note: We've seen two ways in the notes and there is a third possibility that uses the functions from the `infer` package)

```{r}
# Way from notes
mean(pos_samples$correct == "TRUE")

# New way to do this
pos_samples %>%
  specify(formula = correct ~ NULL, success = "TRUE") %>%
  calculate(stat = "prop")
```

**The dog correctly identified 84% of the positive samples.**

<br>

4. This was just one sample. What we're really interested in is how well this dog would do in the long run. What is our parameter of interest?

**p = population proportion of COVID-19 positive samples that the dog can correctly identify**

<br>

5. Generate a bootstrap distribution of sample proportions from 1,000 samples. Create a histogram of your 1,000 sample proportions from this distribution. Since we're going to be using randomization, let's set a seed here so that we can reproduce our results in the future. I've gotten you started (but commented out the code so it will knit).

```{r}
set.seed(1009)
pos_boot <- pos_samples %>%
  specify(formula = correct ~ NULL, success = "TRUE") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop")

visualize(pos_boot)
```

<br>

6. Create a 95% confidence interval using the method of your choice. Write the interval below.

```{r}
pos_ci <- get_ci(pos_boot, level = 0.95, type = "percentile")
pos_ci
```

**95% CI: (`r round(pos_ci, 3)`)**

<br>

7. Provide a sentence interpreting your interval in context of this example.

**We are 95% confident that the true proportion of positive tests that the dog can accurately detect is between 0.786 and 0.888**

<br>

8. Suppose we wanted a narrower interval than the one we got. How could we achieve that?

**Lower the confidence level**

---

# Part 2- Comparing proportions

9. Now let's consider whether the dog does a better job identifying positive or negative specimens. Before we get started, let's make a graph comparing the two. Fill in the blanks below to create a graph with the actual result on the bottom and the dog's result represented with different colors. Try running the graph with and without `position = fill` to see what that argument does. (Note: Remove `eval = FALSE` to run your code)

```{r}
ggplot(covid_dog) +
  geom_bar(aes(x = actual, fill = correct), position = "fill")
```


<br>

10. What does the graph show? What does `position = fill` do and why might we want to include it here?

**The graph shows that in the sample the dog identified a higher proportion of negative samples than positive samples (around 95% vs. 84%). The `position = fill` argument makes the bars the same height so that they display proportions instead of raw counts. **

<br>

11. Clearly the dog didn't do equally well with positive and negative cases in our *sample*, but we want to apply this to a *population.* What is our parameter of interest? Put it in context of this problem.

**$p_1 - p_2$ = the true proportion of negative samples the dog can correctly ID minus the true proportion of positive samples the dog can correctly ID**

<br>

12. Let's calculate our point estimate. We can use the `infer` package to do this and just skip the `generate()` step. Write a sentence explaining what the output of this code represents for our example.

```{r}
dog_pointest <- covid_dog %>%
  specify(formula = correct ~ actual, success = "TRUE") %>%
  calculate(stat = "diff in props", order = c("negative", "positive"))
dog_pointest
```

**In the sample the dog correctly identified negative samples 12 percentage points more often than positive samples.**

<br>

13. Now we need to generate a bootstrap distribution to better understand how this estimate might vary from sample to sample. You can copy and paste your code from above but change `dog_pointest` to a new name and add a `generate()` line in between `specify()` and `calculate()`. Make a histogram of your sample proportions from this distribution. What do you notice about the histogram?

```{r}
dog_boot <- covid_dog %>%
  specify(formula = correct ~ actual, success = "TRUE") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "diff in props", order = c("negative", "positive"))
visualize(dog_boot)
```

**The graph looks bell-shaped. It is centered on 0.12 (our difference in sample proportions). Values range from around 0.05 to 0.20 (all positive). This suggests we will be confident the dog really is better at identifying negative samples than positive samples.**

<br>

14. Generate a 95% confidence interval based on this bootstrap distribution. Write your interval below. The interval you get *should* contain only positive numbers. Explain what that means in the context of this example. (Note: You don't have to write a full interpretation of the interval yet. That will come in the next question.)

```{r}
get_ci(dog_boot, type = "percentile", level = 0.95)
```

**Since the interval contains only positive numbers, I will be 95% confident the dog's true proportion of correct IDs is higher for negative than positive samples**


<br>


15. Interpret the interval in context of the problem. Consider writing the sentence so that it could be included in the article linked at the top of the page and be understood by someone reading that article.

(Reminder: When talking about a difference in percentages, we use "percentage points")

**I am 95% confident that the dog's true percentage of correct identifications is between 6.6 and 17.5 percentage points higher for samples without COVID-19 present than for samples with COVID-19 present.**

<br>


16. The article mentioned the dog might be able to be used at airports or other locations to determine if someone has COVID-19. What might we need to consider about the applicability of these results outside the context of the experiment?

**There are a lot of possible answers here. For example, the dog smelled a small sample, not a person with lots of other smells on them. The dog also was in a relatively distraction free environment. The dog was presented with samples that did and did not contain COVID-19, so it may have been able to compare between them. All of this is to say that what works in a lab setting won't automatically work in a busy setting like an airport under different conditions. This would likely be a first step in showing a dog has the ability to detect COVID-19, but much more study would be needed to see if the dog could continue to do this in other situations.**
